#### 1

- Venus won't let you change this, why? 

  > direct mapping

- cache size

  > block size * number of blocks

##### 1.1

- What combination of parameters is producing the hit rate you observe? (Hint: Your answer should be the process of your calculation.)

  > hit rate: 0
  >
  > cache_size = 32, step_size_in_bytes = 32, so each memory access is mapped to the first cache block.

- What is our hit rate if we increase Rep Count arbitrarily? Why?

  > No change. Data are overwritten in each loop.

- How could we modify our program parameters to maximize our hit rate?

  > Block size = 32
  >
  > Fully associative

##### 1.2

- What combination of parameters is producing the hit rate you observe? (Hint: Your answer should be the process of your calculation.)

  > hit rate: 0.75
  >
  > block_size = 16, # of sets = 4; step_size_in_bytes = 8, iterations_in_a_loop = 32, ...

- What happens to our hit rate as Rep Count goes to infinity? Why?

  > tend to 1. The whole array(size: 256 bytes) is stored in the cache with no block ever overwritten.

- Suppose we have a program that uses a very large array and during each Rep, we apply a different operator to the elements of our array (e.g. if Rep Count = 1024, we apply 1024 different operations to each of the array elements). How can we restructure our program to achieve a hit rate like that achieved in this scenario? (Assume that the number of operations we apply to each element is very large and that the result for each element can be computed independently of the other elements.) What is this technique called?

  > Blocking. Process the array block by block. Finish processing the current block before proceeding to the next.

##### 1.3

- Run the simulation a few times. Every time, set a different seed value (bottom of the cache window). Note that the hit rate is non-deterministic. What is the range of its hit rate? Why is this the case? ("The cache eviction is random" is not a sufficient answer)

  > 0 ~ 0.5
  >
  > 0: every memory access maps to either the 0th or the 8th block, overwriting the previous one. 
  >
  > 0.5: Least Recently used / any policy that maps an access to an unused block in its set. 
  >
  > > step_size_in_bytes = 32, so it hops back and forth between the 1st and the 3rd set. 
  > >
  > > iterations_in_a_loop = 8, so it exactly fills all of the 1st and the 3rd set.

- Which Cache parameter can you modify in order to get a constant hit rate? Record the parameter and its value (and be prepared to show your TA a few runs of the simulation). How does this parameter allow us to get a constant hit rate? And explain why the constant hit rate value is that value.

  > - block size = 256/128/64/
  >
  >   64 â€” the total 256 bytes are exactly partitioned into 4 sets, each of 64 bytes.
  >
  > - associativity = 1
  >
  > - any combination such that $\text{block size} \times \#\text{ of sets} = 256$

  > original seed = 3320754448268424113



#### 2

### 1. Why there is a gap between gb_v and gb_h ?

From the base test, we get that the performance of gb_h is better than gb_v. So we can conclude that the data is more continuous in the horizontal direction. So the data is more continuous in the horizontal direction, so the performance will be better even two more transpose operations are added.

### 2. Why the changed execution order will achieve a better performance even if we do more things(transpose)?

As the transpose operation is a fast enough operation, and as the question 1 answered, after the transpose operation, the data is more continuous, so the performance will be better even two more transpose operations are added.

> row major



#### 3

There is a theroem in struct called cache alignment. As int type under linux x64 is 4 bytes and long type is 8 bytes, so we can first define 2 int and then 1 long type to make sure the 2 int are in the same cache line. Then as for the long array char, we define them as the last, so that they will noe effected the cache line of the previous 2 int and the long type.